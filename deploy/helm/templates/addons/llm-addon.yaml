apiVersion: extensions.kubeblocks.io/v1alpha1
kind: Addon
metadata:
  name: llm
  labels:
    {{- include "kubeblocks.labels" . | nindent 4 }}
    "kubeblocks.io/provider": community
  {{- if .Values.keepAddons }}
  annotations:
    helm.sh/resource-policy: keep
  {{- end }}
spec:
  description: 'vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.'

  type: Helm

  helm:
    {{- include "kubeblocks.addonChartLocationURL" ( dict "name" "llm" "version" (default .Chart.Version .Values.versionOverride) "values" .Values) | indent 4 }}
    {{- include "kubeblocks.addonChartsImage" . | indent 4 }}

    installOptions:
     {{- if hasPrefix "oci://" .Values.addonChartLocationBase }}
      version: {{ default .Chart.Version .Values.versionOverride }}
     {{- end }}

  installable:
    autoInstall: false

  defaultInstallValues:
    - enabled: false
