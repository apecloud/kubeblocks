# Default values for kubeblocks
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Enabled this if you wish to install controller deployment
replicaCount: 1

image:
  repository: docker.io/apecloud/kubeblocks
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

# runtime:
  # cri type: auto/containerd/docker
  # criType: auto
  # /var/run/docker.sock
  # runtimeEndpoint:

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

loggerSettings:
  # Development Mode defaults(encoder=consoleEncoder,logLevel=Debug,stackTraceLevel=Warn).
  # Production Mode defaults(encoder=jsonEncoder,logLevel=Info,stackTraceLevel=Error) (default false)
  developmentMode: false
  # log encoding (one of 'json' or 'console')
  encoder: console
  # log level, can be one of 'debug', 'info', 'error', or any integer value > 0 
  # which corresponds to custom debug levels of increasing verbosity.
  level:
  # Zap time encoding (one of 'epoch', 'millis', 'nano', 'iso8601', 'rfc3339' or 
  # 'rfc3339nano'). Defaults to 'iso8601'.
  timeEncoding: 'iso8601'

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL

podSecurityContext:
  runAsNonRoot: true
  # readOnlyRootFilesystem: true
  # runAsUser: 1000
  # fsGroup: 2000
  # TODO(user): For common cases that do not require escalating privileges
  # it is recommended to ensure that all your Pods/Containers are restrictive.
  # More info: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
  # Please uncomment the following code if your project does NOT have to work on old Kubernetes
  # versions < 1.19 or on vendors versions which do NOT support this field by default (i.e. Openshift < 4.11 ).
  # seccompProfile:
  #   type: RuntimeDefault

service:
  type: ClusterIP
  port: 9443
  # -- Service node port.
  # Only used if `service.type` is `NodePort`.
  nodePort:


## Metrics serviceMonitor parameters
## Enable this if you're using Prometheus Operator
##
serviceMonitor:
  enabled: false
  # metrics server will be exposed at this port.
  port: 8080
  # Only used if `service.type` is `NodePort`.
  nodePort:

# -- Topology spread constraints.
topologySpreadConstraints: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # TODO(user): Configure the resources accordingly based on the project requirements.
  # More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # limits:
  #   cpu: 500m
  #   memory: 128Mi
  # requests:
  #   cpu: 10m
  #   memory: 64Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

podDisruptionBudget:
  minAvailable: 1
  maxUnavailable:

admissionWebhooks:
  enabled: true
  createSelfSignedCert: true

dashboards:
  ## If false, dashboards will not be installed
  ##
  enabled: false

## Data protection settings
##
dataProtection:
  ## @param dataProtection.enableVolumeSnapshot - set this to true if cluster does have snapshot.storage.k8s.io API installed
  ##
  enableVolumeSnapshot: false
  ## @param dataProtection.backupSchedule -- set backup policy schedule time
  ## backupSchedule is in Cron format, the timezone is in UTC. see https://en.wikipedia.org/wiki/Cron.
  ## Example: 0 2 * * *  -- backup job will be scheduled to start at 2:00 each day.
  ##
  backupSchedule: ""
  ## @param dataProtection.backupTTL -- set backup time to live.
  ## backupTTL is a time.Duration-parseable string describing how long.
  ## Example: 168h0m0s  -- the backup will expire in 7 days.
  ##
  backupTTL: ""


# Sub-charts
prometheus:
  ## If false, prometheus sub-chart will not be installed
  ##
  enabled: false

  alertmanager:
    ## If false, alertmanager will not be installed
    ##
    enabled: true

    ## alertmanager container image
    ##
    image:
      repository: docker.io/apecloud/alertmanager
      tag: v0.24.0

    persistentVolume:
      ## If true, alertmanager will create/use a Persistent Volume Claim
      ## If false, use emptyDir
      ##
      enabled: true

      ## alertmanager data Persistent Volume size
      ##
      size: 1Gi

      ## alertmanager data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"

    ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
    ##
    replicaCount: 1

    statefulSet:
      ## If true, use a statefulset instead of a deployment for pod management.
      ## This allows to scale replicas to more than 1 pod
      ##
      enabled: true

      ## Alertmanager headless service to use for the statefulset
      ##
      headless:
        ## Enabling peer mesh service end points for enabling the HA alert manager
        ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
        enableMeshPeer: true

    ## alertmanager resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
      # limits:
      #   cpu: 10m
      #   memory: 32Mi
      # requests:
      #   cpu: 10m
      #   memory: 32Mi

  kubeStateMetrics:
    ## If false, kube-state-metrics sub-chart will not be installed
    ##
    enabled: false

  nodeExporter:
    ## If false, node-exporter will not be installed
    ##
    enabled: false

    ## node-exporter container image
    ##
    image:
      repository: docker.io/apecloud/node-exporter
      tag: v1.3.1

  server:
    ## Prometheus server container name
    ##
    enabled: true

    ## Prometheus server container image
    ##
    image:
      repository: docker.io/apecloud/prometheus
      tag: v2.39.1

    global:
      ## How frequently to scrape targets by default
      ##
      scrape_interval: 15s
      ## How long until a scrape request times out
      ##
      scrape_timeout: 10s
      ## How frequently to evaluate rules
      ##
      evaluation_interval: 15s

    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
    ##
    remoteWrite: []

    ## Prefix used to register routes, overriding externalUrl route.
    ## Useful for proxies that rewrite URLs.
    ##
    routePrefix: /

    persistentVolume:
      ## If true, Prometheus server will create/use a Persistent Volume Claim
      ## If false, use emptyDir
      ##
      enabled: true

      ## Prometheus server data Persistent Volume size
      ##
      size: 8Gi

      ## Prometheus server data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"

    ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)
    ##
    replicaCount: 1

    statefulSet:
      ## If true, use a statefulset instead of a deployment for pod management.
      ## This allows to scale replicas to more than 1 pod
      ##
      enabled: true

    ## Prometheus server resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
      # limits:
      #   cpu: 500m
      #   memory: 512Mi
      # requests:
      #   cpu: 500m
      #   memory: 512Mi

    ## Prometheus' data retention period (default if not specified is 15 days)
    ##
    retention: "3d"

  ## Sample prometheus rules/alerts
  ## NOTE: Please review these carefully as thresholds and behavior may not meet
  ##       your SLOs or labels.
  ##
  ruleFiles:
    cadvisor_alert_rules.yml: |
      groups:
        - name: GoogleCadvisor
          rules:
            - alert: ContainerKilled
              expr: 'time() - container_last_seen{container!="",container!="POD"} > 60'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: "Container killed (node: {{ $labels.instance }}, pod: {{ $labels.pod }}, container: {{ $labels.container }})"
                description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ContainerCpuUsageWarning
              expr: 'sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[2m])) BY (instance,pod,container) * 100 > 70'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "Container CPU usage is high (> 70%) (node: {{ $labels.instance }}, pod: {{ $labels.pod }}, container: {{ $labels.container }})"
                description: "Container CPU usage is above 70%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ContainerCpuUsageCritical
              expr: 'sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[2m])) BY (instance,pod,container) * 100 > 90'
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: "Container CPU usage is very high (> 90%) (node: {{ $labels.instance }}, pod: {{ $labels.pod }}, container: {{ $labels.container }})"
                description: "Container CPU usage is above 90%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ContainerMemoryUsage
              expr: 'sum(container_memory_working_set_bytes{container!="",container!="POD"}) BY (instance,pod,container) / sum(container_spec_memory_limit_bytes{container!="",container!="POD"} > 0) BY (instance,pod,container) * 100 > 90'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "Container Memory usage is high (> 90%) (node: {{ $labels.instance }}, pod: {{ $labels.pod }}, container: {{ $labels.container }})"
                description: "Container Memory usage is above 90%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ContainerMemoryUsagePredict
              expr: 'sum(predict_linear(container_memory_working_set_bytes{container!="",container!="POD"}[15m], 30*60)) BY (instance,pod,container) - sum(container_spec_memory_limit_bytes{container!="",container!="POD"} > 0) BY (instance,pod,container) >= 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: "Container Memory usage may exceed the limit 30 minutes later (node: {{ $labels.instance }}, pod: {{ $labels.pod }}, container: {{ $labels.container }})"
                description: "Container Memory usage may exceed the limit 30 minutes later\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ContainerVolumeUsage
              expr: 'sum(container_fs_usage_bytes) BY (instance,device) / sum(container_fs_limit_bytes) BY (instance,device) * 100 > 90'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "Device Volume usage is high (> 90%) (node: {{ $labels.instance }}, device: {{ $labels.device }})"
                description: "Device Volume usage is above 90%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: ContainerHighCpuThrottleRate
              expr: 'rate(container_cpu_cfs_throttled_seconds_total{container!="",container!="POD"}[2m]) > 1'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "Container high throttle rate (node: {{ $labels.instance }}, pod: {{ $labels.pod }}, container: {{ $labels.container }})"
                description: "Container is being throttled\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    mysql_alert_rules.yml: |
      groups:
        - name: MysqldExporter
          rules:
            - alert: MysqlDown
              expr: 'max_over_time(mysql_up[1m]) == 0'
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: "MySQL is down (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "MySQL instance is down on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlRestarted
              expr: 'mysql_global_status_uptime < 60'
              for: 0m
              labels:
                severity: info
              annotations:
                summary: "MySQL restarted (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "MySQL has just been restarted, less than one minute ago on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlTooManyConnections
              expr: 'sum(max_over_time(mysql_global_status_threads_connected[1m]) / mysql_global_variables_max_connections) BY (namespace,app_kubernetes_io_instance,pod) * 100 > 80'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "MySQL too many connections (> 80%) (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "More than 80% of MySQL connections are in use on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlConnectionErrors
              expr: 'sum(increase(mysql_global_status_connection_errors_total[1m])) BY (namespace,app_kubernetes_io_instance,pod) > 0'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "MySQL connection errors (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "MySQL server has some connection errors on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlHighThreadsRunning
              expr: 'sum(max_over_time(mysql_global_status_threads_running[1m]) / mysql_global_variables_max_connections) BY (namespace,app_kubernetes_io_instance,pod) * 100 > 60'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "MySQL high threads running (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "More than 60% of MySQL connections are in running state on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlSlowQueries
              expr: 'sum(increase(mysql_global_status_slow_queries[1m])) BY (namespace,app_kubernetes_io_instance,pod) > 0'
              for: 2m
              labels:
                severity: info
              annotations:
                summary: "MySQL slow queries (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "MySQL server has some new slow query on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlInnodbLogWaits
              expr: 'sum(rate(mysql_global_status_innodb_log_waits[5m])) BY (namespace,app_kubernetes_io_instance,pod) > 10'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "MySQL InnoDB log waits (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "MySQL innodb log writes stalling on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: MysqlInnodbBufferPoolHits
              expr: 'sum(rate(mysql_global_status_innodb_buffer_pool_reads[5m]) / rate(mysql_global_status_innodb_buffer_pool_read_requests[5m])) BY (namespace,app_kubernetes_io_instance,pod) * 100 > 5'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "MySQL InnoDB high read requests rate hitting disk (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "High number of logical reads that InnoDB could not satisfy from the buffer pool, and had to read directly from disk on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    postgresql_alert_rules.yml: |
      groups:
        - name: PostgreSQLExporter
          rules:
            - alert: PostgreSQLDown
              expr: 'max_over_time(pg_up[1m]) == 0'
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: "PostgreSQL is down (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL instance is down on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLExporterError
              expr: 'pg_exporter_last_scrape_error > 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL exporter scrape error (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL exporter is showing errors. A query may be buggy in query.yaml\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLTooManySlowQueries
              expr: |
                max by(namespace,app_kubernetes_io_instance,pod,datname) (
                  max_over_time(pg_stat_activity_max_tx_duration{datname!~"template.*|postgres"}[2m])
                ) > 60
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL database {{ $labels.datname }} high number of slow queries (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL high number of slow queries\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLTooManyConnections
              expr: |
                sum by (namespace,app_kubernetes_io_instance,pod) (pg_stat_activity_count{datname!~"template.*|postgres"})
                > on(namespace,app_kubernetes_io_instance,pod)
                (pg_settings_max_connections - pg_settings_superuser_reserved_connections) * 0.8
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL too many connections (> 80%) (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL instance has too many connections (> 80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLDeadLocks
              expr: 'increase(pg_stat_database_deadlocks{datname!~"template.*|postgres", datname!=""}[2m]) > 5'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL database {{ $labels.datname }} dead locks (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL has deadlocks\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLHighRollbackRate
              expr: |
                rate(pg_stat_database_xact_rollback{datname!~"template.*|postgres", datname!=""}[2m])
                /
                rate(pg_stat_database_xact_commit{datname!~"template.*|postgres", datname!=""}[2m])
                > 0.1
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL database {{ $labels.datname }} high rollback rate (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "Ratio of transactions being aborted compared to committed is > 2%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLTooManyLocksAcquired
              expr: |
                sum by (namespace,app_kubernetes_io_instance,pod) (pg_locks_count)
                / on(namespace,app_kubernetes_io_instance,pod)
                (pg_settings_max_locks_per_transaction * pg_settings_max_connections)
                > 0.2
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL too many locks acquired (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "Too many locks acquired on the database. If this alert happens frequently, we may need to increase the postgres setting max_locks_per_transaction.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLCacheHitRatio
              expr: |
                avg by (namespace,app_kubernetes_io_instance,pod,datname) (
                  rate(pg_stat_database_blks_hit{datname!~"template.*|postgres", datname!=""}[2m])
                  /
                  (
                    rate(
                      pg_stat_database_blks_hit{datname!~"template.*|postgres", datname!=""}[2m]
                    )
                    +
                    rate(
                      pg_stat_database_blks_read{datname!~"template.*|postgres", datname!=""}[2m]
                    )
                  )
                ) < 0.9
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL database {{ $labels.datname }} has low cache hit rate (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "Low cache hit rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLMaxWriteBufferReached
              expr: 'rate(pg_stat_bgwriter_maxwritten_clean_total[2m]) > 0'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL write buffers reached max (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL background writer stops for max\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PostgreSQLHighWALFilesArchiveErrorRate
              expr: |
                rate(pg_stat_archiver_failed_count[2m])
                / (
                  rate(pg_stat_archiver_archived_count[2m]) + rate(pg_stat_archiver_failed_count[2m])
                ) > 0.1
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: "PostgreSQL high error rate in WAL files archiver (namespace: {{ $labels.namespace }}, cluster: {{ $labels.app_kubernetes_io_instance }}, instance: {{ $labels.pod }})"
                description: "PostgreSQL high error rate in WAL files archiver\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  serverFiles:
    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
        - /etc/config/cadvisor_alert_rules.yml
        - /etc/config/mysql_alert_rules.yml
        - /etc/config/postgresql_alert_rules.yml

      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090

        # Scrape config for API servers.
        #
        # Kubernetes exposes API servers as endpoints to the default/kubernetes
        # service so this uses `endpoints` role and uses relabelling to only keep
        # the endpoints associated with the default/kubernetes service using the
        # default named port `https`. This works for single API server deployments as
        # well as HA API server deployments.
        - job_name: 'kubernetes-apiservers'

          kubernetes_sd_configs:
            - role: endpoints

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          relabel_configs:
            - source_labels: [ __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name ]
              action: keep
              regex: default;kubernetes;https

        - job_name: 'kubernetes-nodes'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [ __meta_kubernetes_node_name ]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$1/proxy/metrics

        - job_name: 'kubernetes-nodes-cadvisor'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node

          # This configuration will work only on kubelet 1.7.3+
          # As the scrape endpoints for cAdvisor have changed
          # if you are using older version you need to change the replacement to
          # replacement: /api/v1/nodes/$1:4194/proxy/metrics
          # more info here https://github.com/coreos/prometheus-operator/issues/633
          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [ __meta_kubernetes_node_name ]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

        # Example scrape config for pods
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,
        # except if `prometheus.io/scrape-slow` is set to `true` as well.
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods'
          honor_labels: true

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape ]
              action: keep
              regex: true
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow ]
              action: drop
              regex: true
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scheme ]
              action: replace
              regex: (https?)
              target_label: __scheme__
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_path ]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
              action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labeldrop
              regex: __meta_kubernetes_pod_label_controller_(.+)
            - action: labeldrop
              regex: __meta_kubernetes_pod_label_statefulset_(.+)
            - action: labeldrop
              regex: __meta_kubernetes_pod_label_cs_(.+)
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [ __meta_kubernetes_namespace ]
              action: replace
              target_label: namespace
            - source_labels: [ __meta_kubernetes_pod_name ]
              action: replace
              target_label: pod
            - source_labels: [ __meta_kubernetes_pod_phase ]
              regex: Pending|Succeeded|Failed|Completed
              action: drop

        # Example Scrape config for pods which should be scraped slower. An useful example
        # would be stackriver-exporter which queries an API on every scrape of the pod
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods-slow'
          honor_labels: true

          scrape_interval: 5m
          scrape_timeout: 30s

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow ]
              action: keep
              regex: true
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scheme ]
              action: replace
              regex: (https?)
              target_label: __scheme__
            - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_path ]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
              action: replace
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labeldrop
              regex: __meta_kubernetes_pod_label_controller_(.+)
            - action: labeldrop
              regex: __meta_kubernetes_pod_label_statefulset_(.+)
            - action: labeldrop
              regex: __meta_kubernetes_pod_label_cs_(.+)
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [ __meta_kubernetes_namespace ]
              action: replace
              target_label: namespace
            - source_labels: [ __meta_kubernetes_pod_name ]
              action: replace
              target_label: pod
            - source_labels: [ __meta_kubernetes_pod_phase ]
              regex: Pending|Succeeded|Failed|Completed
              action: drop

  pushgateway:
    ## If false, pushgateway will not be installed
    ##
    enabled: false

grafana:
  ## If false, grafana sub-chart will not be installed
  ##
  enabled: false

  replicas: 1

  ## Grafana server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  ## Timezone for the default dashboards
  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
  ##
  defaultDashboardsTimezone:

  adminUser: admin
  adminPassword: abc

  sidecar:
    image:
      repository: docker.io/apecloud/k8s-sidecar
      tag: 1.19.2

    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"

    datasources:
      enabled: true
      label: grafana_datasource
      labelValue: "1"

      defaultDatasourceEnabled: true
      uid: prometheus

      skipReload: true
      initDatasources: true

  testFramework:
    enabled: false

  grafana.ini:
    # Basic auth is enabled by default and works with the builtin Grafana user password authentication system and LDAP authentication integration.
    auth.basic:
      enabled: false

    auth.anonymous:
      enabled: true
      # Hide the Grafana version text from the footer and help tooltip for unauthenticated users (default: false)
      hide_version: true

loadbalancer:
  enabled: false


apecloud-mysql:
  enabled: true

## snapshot-controller settings
## ref: https://artifacthub.io/packages/helm/piraeus-charts/snapshot-controller#configuration
##
snapshot-controller:
  ## @param snapshot-controller.enabled -- Enable snapshot-controller chart.
  ##
  enabled: false
  ## @param snapshot-controller.replicaCount -- Number of replicas to deploy.
  ##
  replicaCount: 1
  ## snapshot-controller image setting, easy access for CN users.
  ## @param snapshot-controller.image.repository -- Repository to pull the image from.
  ##
  image:
    repository: registry.aliyuncs.com/google_containers/snapshot-controller

## csi-s3 settings
## ref: https://artifacthub.io/packages/helm/cloudve/csi-s3#configuration
##
csi-s3:
  ## @param csi-s3.enabled -- Enable csi-s3 chart.
  enabled: false
  ## csi-s3 secret settings
  ## @param csi-s3.secret.accessKey -- S3 Access Key.
  ## @param csi-s3.secret.secretKey -- S3 Secret Key.
  ## @param csi-s3.secret.endpoint -- S3 Endpoint.
  ##
  secret:
    accessKey: ""
    secretKey: ""
    endpoint: ""
  ## csi-s3 storageClass setting
  ## @param csi-s3.storageClass.create -- Specifies whether the storage class should be created.
  ## @param csi-s3.storageClass.singleBucket -- Use a single bucket for all dynamically provisioned persistent volumes.
  ## @param csi-s3.storageClass.mountOptions -- mount options.
  ##
  storageClass:
    create: true
    singleBucket: ""
    mountOptions: "--memory-limit 1000 --dir-mode 0777 --file-mode 0666 --region cn-northwest-1"
