apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-scripts
  labels:
    {{- include "llm.labels" . | nindent 4 }}
data:
  start.sh: |
    #!/bin/sh
    if [ -n "$CLONE_MODEL_SCRIPT" ]; then
      bash -c "$CLONE_MODEL_SCRIPT"
    fi
    ordinal=${KB_POD_NAME##*-}
    echo "current pod ordinal: $ordinal"
    /scripts/vllm-start.sh &
    if [ $ordinal -eq 0 ]; then
      ray start --head --block
    else 
      ray start --address="${KB_VLLM_0_HOSTNAME}:6379" --block
    fi
  vllm-start.sh: |
    #!/bin/sh
    echo "model=${MODEL_NAME}"
    ordinal=${KB_POD_NAME##*-}
    echo "current pod ordinal: $ordinal"
    echo "EXTRA_ARGS=${EXTRA_ARGS}"
    cd vllm
    while true; do
      if [ $ordinal -eq 0 ]; then
        python -m vllm.entrypoints.api_server --host 0.0.0.0 --port 8000 --model ${MODEL_NAME} --gpu-memory-utilization 0.95 --max-num-seqs 512 --max-num-batched-tokens 8192 --tensor-parallel-size ${KB_VLLM_N} ${EXTRA_ARGS} > log
        code=$?
        if [ $code -eq 0 ]; then
          break
        fi
        echo "exit with code $code, wait for 1 second and try again..."
        sleep 1
      else 
        # keep container running
        sleep 1
      fi
    done
