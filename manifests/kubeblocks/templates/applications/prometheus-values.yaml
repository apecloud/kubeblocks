---
# Source: kubeblocks/templates/applications/prometheus-values.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-chart-kubeblocks-values
  labels:
    helm.sh/chart: kubeblocks-0.8.0-alpha.0
    app.kubernetes.io/name: kubeblocks
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.8.0-alpha.0"
    app.kubernetes.io/managed-by: Helm
data:
  values-kubeblocks-override.yaml: |-
    alertmanager:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: kb-controller
                operator: In
                values:
                - "true"
            weight: 100
      configMapOverrideName: alertmanager-config
      containerSecurityContext:
        allowPrivilegeEscalation: false
      enabled: true
      image:
        repository: infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/alertmanager
        tag: v0.24.0
      ingress:
        annotations: {}
        enabled: false
        extraLabels: {}
        extraPaths: []
        hosts: []
        path: /
        pathType: Prefix
        tls: []
      persistentVolume:
        enabled: false
        size: 1Gi
      replicaCount: 1
      resources: {}
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: false
        runAsUser: 0
      service:
        annotations: {}
        clusterIP: ""
        externalIPs: []
        labels: {}
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 80
        sessionAffinity: None
        type: ClusterIP
      statefulSet:
        enabled: true
        headless:
          enableMeshPeer: true
      tolerations:
      - effect: NoSchedule
        key: kb-controller
        operator: Equal
        value: "true"
    alertmanagerFiles:
      alertmanager.yml:
        global: {}
        receivers:
        - name: default-receiver
        route:
          group_interval: 30s
          group_wait: 5s
          receiver: default-receiver
          repeat_interval: 10m
    configmapReload:
      alertmanager:
        image:
          repository: infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/configmap-reload
          tag: v0.5.0
      prometheus:
        image:
          repository: infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/configmap-reload
          tag: v0.5.0
    enabled: false
    kubeStateMetrics:
      enabled: false
    nodeExporter:
      enabled: false
      image:
        repository: infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/node-exporter
        tag: v1.3.1
    pushgateway:
      enabled: false
    ruleFiles:
      kafka_alert_rules.yaml: |-
        group:
          - name: KafkaExporter
            rules:
              - alert: KafkaTopicsReplicas
                  expr: 'sum(kafka_topic_partition_in_sync_replica) by (topic) < 3'
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: 'Kafka topics replicas (instance {{ $labels.app_kubernetes_io_instance }})'
                    description: 'Kafka topic in-sync partition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}'
              - alert: KafkaConsumersGroup
                  expr: 'sum(kafka_consumergroup_lag) by (consumergroup) > 50'
                  for: 1m
                  labels:
                    severity: critical
                  annotations:
                    summary: 'Kafka consumers group (instance {{ $labels.app_kubernetes_io_instance }})'
                    description: 'Kafka consumers group\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}'
              - alert: KafkaBrokerDown
                  expr: 'kafka_brokers < 3'
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    Summary: 'Kafka broker *{{ $labels.app_kubernetes_io_instance }}* alert status'
                    description: 'One of the Kafka broker *{{ $labels.app_kubernetes_io_instance }}* is down.'
      kubelet_alert_rules.yml: |
        groups:
          - name: KubeletSummary
            rules:
              - alert: ContainerCpuUsageWarning
                expr: 'rate(container_cpu_time_seconds_total[2m]) / container_cpu_limit * 100 > 70'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'Container CPU usage is high (> 70%)'
                  description: 'Container CPU usage is {{ $value | printf "%.2f" }} percent. (pod: {{ $labels.k8s_pod_name }}, container: {{ $labels.k8s_container_name }})'
    
              - alert: ContainerCpuUsageCritical
                expr: 'rate(container_cpu_time_seconds_total[2m]) / container_cpu_limit * 100 > 90'
                for: 1m
                labels:
                  severity: critical
                annotations:
                  summary: 'Container CPU usage is very high (> 90%)'
                  description: 'Container CPU usage is {{ $value | printf "%.2f" }} percent. (pod: {{ $labels.k8s_pod_name }}, container: {{ $labels.k8s_container_name }})'
    
              - alert: ContainerMemoryUsage
                expr: 'container_memory_working_set_bytes / container_memory_limit_bytes * 100 > 90'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'Container Memory usage is high (> 90%)'
                  description: 'Container Memory usage is {{ $value | printf "%.2f" }} percent. (pod: {{ $labels.k8s_pod_name }}, container: {{ $labels.k8s_container_name }})'
    
              - alert: ContainerMemoryUsagePredict
                expr: 'predict_linear(container_memory_working_set_bytes[15m], 30*60) - container_memory_limit_bytes > 0'
                for: 5m
                labels:
                  severity: critical
                annotations:
                  summary: 'Container Memory predict usage may exceed the limit 30 minutes later'
                  description: 'Container Memory predict usage may exceed the limit 30 minutes later, the predict value is {{ $value | humanize1024 }}. (pod: {{ $labels.k8s_pod_name }}, container: {{ $labels.k8s_container_name }})'
    
              - alert: ContainerVolumeUsage
                expr: '(k8s_volume_capacity_bytes - k8s_volume_available_bytes) / k8s_volume_capacity_bytes * 100 > 90'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'Volume usage is high (> 90%)'
                  description: 'Volume usage is {{ $value | printf "%.2f" }} percent. (pod: {{ $labels.k8s_pod_name }}, volume: {{ $labels.k8s_volume_name }})'
      mongodb_alert_rules.yml: "groups:\n  - name: MongodbExporter\n    rules:\n      -
        alert: MongodbDown\n        expr: 'max_over_time(mongodb_up[1m]) == 0'\n        for:
        0m\n        labels:\n          severity: critical\n        annotations:\n          summary:
        'MongoDB is Down'\n          description: 'MongoDB instance is down\\n  VALUE
        = {{ $value }}\\n  LABELS = {{ $labels }}'\n\n      - alert: MongodbRestarted\n
        \       expr: 'mongodb_instance_uptime_seconds < 60'\n        for: 0m\n        labels:\n
        \         severity: info\n        annotations:\n          summary: 'Mongodb has
        just been restarted (< 60s)'\n          description: 'Mongodb has just been restarted
        {{ $value | printf \"%.1f\" }} seconds ago\\n LABELS = {{ $labels }}'\n\n      -
        alert: MongodbReplicaMemberUnhealthy\n        expr: 'max_over_time(mongodb_rs_members_health[1m])
        == 0'\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n
        \         summary: 'Mongodb replica member is unhealthy'\n          description:
        'MongoDB replica member is not healthy\\n  VALUE = {{ $value }}\\n  LABELS = {{
        $labels }}'\n\n      - alert: MongodbReplicationLag\n        expr: '(mongodb_rs_members_optimeDate{member_state=\"PRIMARY\"}
        - on (pod) group_right mongodb_rs_members_optimeDate{member_state=\"SECONDARY\"})
        / 1000 > 10'\n        for: 0m\n        labels:\n          severity: critical\n
        \       annotations:\n          summary: 'MongoDB replication lag (> 10s)'\n          description:
        'Mongodb replication lag is more than 10s\\n  VALUE = {{ $value }}\\n  LABELS
        = {{ $labels }}'\n\n      - alert: MongodbReplicationHeadroom\n        expr: 'sum(avg(mongodb_mongod_replset_oplog_head_timestamp
        - mongodb_mongod_replset_oplog_tail_timestamp)) - sum(avg(mongodb_rs_members_optimeDate{member_state=\"PRIMARY\"}
        - on (pod) group_right mongodb_rs_members_optimeDate{member_state=\"SECONDARY\"}))
        <= 0'\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n
        \         summary: 'MongoDB replication headroom (< 0)'\n          description:
        'MongoDB replication headroom is <= 0\\n  VALUE = {{ $value }}\\n  LABELS = {{
        $labels }}'\n\n      - alert: MongodbNumberCursorsOpen\n        expr: 'mongodb_ss_metrics_cursor_open{csr_type=\"total\"}
        > 10 * 1000'\n        for: 2m\n        labels:\n          severity: warning\n
        \       annotations:\n          summary: 'MongoDB opened cursors num (> 10k)'\n
        \         description: 'Too many cursors opened by MongoDB for clients (> 10k)\\n
        \ VALUE = {{ $value }}\\n  LABELS = {{ $labels }}'\n\n      - alert: MongodbCursorsTimeouts\n
        \       expr: 'increase(mongodb_ss_metrics_cursor_timedOut[1m]) > 100'\n        for:
        2m\n        labels:\n          severity: warning\n        annotations:\n          summary:
        'MongoDB cursors timeouts (>100/minute)' \n          description: 'Too many cursors
        are timing out (> 100/minute)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels
        }}'\n\n      - alert: MongodbTooManyConnections\n        expr: 'avg by(pod) (rate(mongodb_ss_connections{conn_type=\"current\"}[1m]))
        / avg by(pod) (sum (mongodb_ss_connections) by(pod)) * 100 > 80'\n        for:
        2m\n        labels:\n          severity: warning\n        annotations:\n          summary:
        'MongoDB too many connections (> 80%)'\n          description: 'Too many connections
        (> 80%)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}'\n\n      - alert:
        MongodbVirtualMemoryUsage\n        expr: '(sum(mongodb_ss_mem_virtual) BY (pod)
        / sum(mongodb_ss_mem_resident) BY (pod)) > 100'\n        for: 2m\n        labels:\n
        \         severity: warning\n        annotations:\n          summary: MongoDB
        virtual memory usage high\n          description: \"High memory usage: the quotient
        of (mem_virtual / mem_resident) is more than 100\\n  VALUE = {{ $value }}\\n  LABELS
        = {{ $labels }}\""
      mysql_alert_rules.yml: |
        groups:
          - name: MysqldExporter
            rules:
              - alert: MysqlDown
                expr: 'max_over_time(mysql_up[1m]) == 0'
                for: 0m
                labels:
                  severity: critical
                annotations:
                  summary: 'MySQL is down'
                  description: 'MySQL is down. (instance: {{ $labels.pod }})'
    
              - alert: MysqlRestarted
                expr: 'mysql_global_status_uptime < 60'
                for: 0m
                labels:
                  severity: info
                annotations:
                  summary: 'MySQL has just been restarted (< 60s)'
                  description: 'MySQL has just been restarted {{ $value | printf "%.1f" }} seconds ago. (instance: {{ $labels.pod }})'
    
              - alert: MysqlTooManyConnections
                expr: 'sum(max_over_time(mysql_global_status_threads_connected[1m]) / mysql_global_variables_max_connections) BY (namespace,app_kubernetes_io_instance,pod) * 100 > 80'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'MySQL has too many connections (> 80%)'
                  description: '{{ $value | printf "%.2f" }} percent of MySQL connections are in use. (instance: {{ $labels.pod }})'
    
              - alert: MysqlConnectionErrors
                expr: 'sum(increase(mysql_global_status_connection_errors_total[1m])) BY (namespace,app_kubernetes_io_instance,pod) > 0'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'MySQL connection errors'
                  description: 'MySQL has connection errors and the value is {{ $value | printf "%.2f" }}. (instance: {{ $labels.pod }})'
    
              - alert: MysqlHighThreadsRunning
                expr: 'sum(max_over_time(mysql_global_status_threads_running[1m]) / mysql_global_variables_max_connections) BY (namespace,app_kubernetes_io_instance,pod) * 100 > 60'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'MySQL high threads running (> 60%)'
                  description: '{{ $value | printf "%.2f" }} percent of MySQL connections are in running state. (instance: {{ $labels.pod }})'
    
              - alert: MysqlSlowQueries
                expr: 'sum(increase(mysql_global_status_slow_queries[1m])) BY (namespace,app_kubernetes_io_instance,pod) > 0'
                for: 2m
                labels:
                  severity: info
                annotations:
                  summary: 'MySQL slow queries'
                  description: 'MySQL server has {{ $value | printf "%.2f" }} slow query. (instance: {{ $labels.pod }})'
    
              - alert: MysqlInnodbLogWaits
                expr: 'sum(rate(mysql_global_status_innodb_log_waits[5m])) BY (namespace,app_kubernetes_io_instance,pod) > 10'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'MySQL InnoDB log waits (> 10)'
                  description: 'MySQL innodb log writes stalling and the value is {{ $value | printf "%.2f" }}. (instance: {{ $labels.pod }})'
    
              - alert: MysqlInnodbBufferPoolHits
                expr: 'sum(rate(mysql_global_status_innodb_buffer_pool_reads[5m]) / rate(mysql_global_status_innodb_buffer_pool_read_requests[5m])) BY (namespace,app_kubernetes_io_instance,pod) * 100 > 5'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'MySQL InnoDB high read requests rate hitting disk (> 5%)'
                  description: 'High number of logical reads that InnoDB could not satisfy from the buffer pool, and had to read directly from disk. The value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
      postgresql_alert_rules.yml: |
        groups:
          - name: PostgreSQLExporter
            rules:
              - alert: PostgreSQLDown
                expr: 'max_over_time(pg_up[1m]) == 0'
                for: 0m
                labels:
                  severity: critical
                annotations:
                  summary: 'PostgreSQL is down'
                  description: 'PostgreSQL is down. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLRestarted
                expr: 'time() - pg_postmaster_start_time_seconds < 60'
                for: 0m
                labels:
                  severity: info
                annotations:
                  summary: 'PostgreSQL has just been restarted (< 60s)'
                  description: 'PostgreSQL has just been restarted {{ $value | printf "%.1f" }} seconds ago. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLExporterError
                expr: 'pg_exporter_last_scrape_error > 0'
                for: 0m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL exporter scrape error'
                  description: 'PostgreSQL exporter has {{ $value | printf "%.2f" }} scrape errors. A query may be buggy in query.yaml. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLTooManySlowQueries
                expr: |
                  max by(namespace,app_kubernetes_io_instance,pod,datname) (
                    max_over_time(pg_stat_activity_max_tx_duration{datname!~"template.*"}[2m])
                  ) > 60
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL database has high number of slow queries'
                  description: 'PostgreSQL database has slow queries and the value is {{ $value | printf "%.2f" }}. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
    
              - alert: PostgreSQLTooManyConnections
                expr: |
                  sum by (namespace,app_kubernetes_io_instance,pod) (pg_stat_activity_count{datname!~"template.*"})
                  > on(namespace,app_kubernetes_io_instance,pod)
                  (pg_settings_max_connections - pg_settings_superuser_reserved_connections) * 0.8
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL too many connections (> 80%)'
                  description: 'PostgreSQL has too many connections and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLDeadLocks
                expr: 'increase(pg_stat_database_deadlocks_total{datname!~"template.*", datname!=""}[2m]) > 5'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL database has dead locks (> 5)'
                  description: 'PostgreSQL database has {{ $value | printf "%.2f"}} dead locks. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
    
              - alert: PostgreSQLHighRollbackRate
                expr: |
                  rate(pg_stat_database_xact_rollback_total{datname!~"template.*", datname!=""}[2m])
                  /
                  rate(pg_stat_database_xact_commit_total{datname!~"template.*", datname!=""}[2m])
                  > 0.1
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL database has high rollback rate (> 10%)'
                  description: 'Ratio of transactions being aborted compared to committed is {{ $value | printf "%.2f"}} percent. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
    
              - alert: PostgreSQLTooManyLocksAcquired
                expr: |
                  sum by (namespace,app_kubernetes_io_instance,pod) (pg_locks_count)
                  / on(namespace,app_kubernetes_io_instance,pod)
                  (pg_settings_max_locks_per_transaction * pg_settings_max_connections)
                  > 0.2
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL has too many locks acquired (> 20%)'
                  description: 'Too many locks acquired on the database and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLCacheHitRatio
                expr: |
                  avg by (namespace,app_kubernetes_io_instance,pod,datname) (
                    rate(pg_stat_database_blks_hit_total{datname!~"template.*", datname!=""}[2m])
                    /
                    (
                      rate(
                        pg_stat_database_blks_hit_total{datname!~"template.*", datname!=""}[2m]
                      )
                      +
                      rate(
                        pg_stat_database_blks_read_total{datname!~"template.*", datname!=""}[2m]
                      )
                    )
                  ) < 0.9
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL database has low cache hit rate (< 90%)'
                  description: 'Low cache hit rate and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
    
              - alert: PostgreSQLMaxWriteBufferReached
                expr: 'rate(pg_stat_bgwriter_maxwritten_clean_total[2m]) > 0'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL write buffers reached max'
                  description: 'PostgreSQL background writer stops for max and the value is {{ $value | printf "%.2f" }}. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLHighWALFilesArchiveErrorRate
                expr: |
                  rate(pg_stat_archiver_failed_count_total[2m])
                  / (
                    rate(pg_stat_archiver_archived_count_total[2m]) + rate(pg_stat_archiver_failed_count_total[2m])
                  ) > 0.1
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL has high error rate in WAL files archiver(> 10%)'
                  description: 'PostgreSQL high error rate in WAL files archiver and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
    
              - alert: PostgreSQLTableNotAutoVacuumed
                expr: |
                  (pg_stat_user_tables_last_autovacuum > 0)
                  and
                  (time() - pg_stat_user_tables_last_autovacuum)
                  > 24 * 60 * 60 * 10
                for: 0m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL table in database has not been auto vacuumed for 10 days'
                  description: 'Table {{ $labels.relname }} in database has not been auto vacuumed for 10 days. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
    
              - alert: PostgreSQLTableNotAutoAnalyzed
                expr: |
                  (pg_stat_user_tables_last_autoanalyze > 0)
                  and
                  (time() - pg_stat_user_tables_last_autoanalyze)
                  > 24 * 60 * 60 * 10
                for: 0m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL table in database has not been auto analyzed for 10 days'
                  description: 'Table {{ $labels.relname }} in database has not been auto analyzed for 10 days. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
    
              - alert: PostgreSQLTableTooManyDeadTuples
                expr: |
                  (pg_stat_user_tables_n_dead_tup > 10000)
                  /
                  (pg_stat_user_tables_n_live_tup + pg_stat_user_tables_n_dead_tup)
                  >= 0.1
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'PostgreSQL table in database has too many dead tuples (> 10%)'
                  description: 'Table {{ $labels.relname }} in database dead tuples is too large and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }}, database: {{ $labels.datname }})'
      redis_alert_rules.yml: |
        groups:
          - name: RedisExporter
            rules:
              - alert: RedisDown
                expr: 'redis_up == 0'
                for: 5m
                labels:
                  severity: critical
                annotations:
                  summary: 'Redis is down'
                  description: 'Redis is down. (instance: {{ $labels.pod }})'
    
              - alert: RedisCPUHigh
                expr: '(rate(redis_cpu_sys_seconds_total[1m]) + rate(redis_cpu_user_seconds_total[1m])) * 100 > 80'
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: 'Out of CPU (> 80%)'
                  description: 'Redis is running out of CPU and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
    
              - alert: RedisMemoryHigh
                expr: '(redis_memory_max_bytes == 0 or redis_memory_used_bytes * 100 / redis_memory_max_bytes) > 90'
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: 'Out of memory (> 90%)'
                  description: 'Redis is running out of memory and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
    
              - alert: RedisTooManyConnections
                expr: 'redis_connected_clients * 100 / redis_config_maxclients > 80'
                for: 1m
                labels:
                  severity: warning
                annotations:
                  summary: 'Redis has too many connections (> 80%)'
                  description: 'Redis has too many connections and the value is {{ $value | printf "%.2f" }} percent. (instance: {{ $labels.pod }})'
    
              - alert: RedisRejectedConnections
                expr: 'increase(redis_rejected_connections_total[1m]) > 0'
                for: 5m
                labels:
                  severity: error
                annotations:
                  summary: 'Redis has rejected connections'
                  description: '{{ $value | printf "%.2f" }} connections to Redis has been rejected. (instance: {{ $labels.pod }})'
    
              - alert: RedisKeyEviction
                expr: 'increase(redis_evicted_keys_total[5m]) > 0'
                for: 1s
                labels:
                  severity: error
                annotations:
                  summary: 'Redis has evicted keys'
                  description: 'Redis has evicted keys in the last 5 minutes and the value is {{ $value | printf "%.2f" }}. (instance: {{ $labels.pod }})'
    
              - alert: RedisMissingMaster
                expr: 'count by (app_kubernetes_io_instance) (redis_instance_info{role="master"}) < 1'
                for: 30s
                labels:
                  severity: critical
                annotations:
                  summary: 'Redis missing master'
                  description: 'Redis cluster has no node marked as master.'
    
              - alert: RedisDisconnectedSlaves
                expr: 'count without (instance, job) (redis_connected_slaves) - sum without (instance, job) (redis_connected_slaves) - 1 > 1'
                for: 0m
                labels:
                  severity: critical
                annotations:
                  summary: 'Redis disconnected slaves'
                  description: 'Redis not replicating for all slaves. Consider reviewing the redis replication status. (instance: {{ $labels.pod }})'
    
              - alert: RedisReplicationBroken
                expr: 'delta(redis_connected_slaves[1m]) < 0'
                for: 0m
                labels:
                  severity: critical
                annotations:
                  summary: 'Redis replication broken'
                  description: 'Redis instance lost a slave. (instance: {{ $labels.pod }})'
    server:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: kb-controller
                operator: In
                values:
                - "true"
            weight: 100
      containerSecurityContext:
        allowPrivilegeEscalation: false
      enabled: true
      extraArgs:
        enable-feature: memory-snapshot-on-shutdown
        log.level: info
        storage.tsdb.min-block-duration: 30m
        storage.tsdb.retention.size: 10GB
      extraFlags:
      - web.enable-lifecycle
      - web.enable-remote-write-receiver
      global:
        evaluation_interval: 15s
        scrape_interval: 15s
        scrape_timeout: 10s
      image:
        repository: infracreate-registry.cn-zhangjiakou.cr.aliyuncs.com/apecloud/prometheus
        tag: v2.44.0
      ingress:
        annotations: {}
        enabled: false
        extraLabels: {}
        extraPaths: []
        hosts: []
        path: /
        pathType: Prefix
        tls: []
      persistentVolume:
        enabled: false
        size: 20Gi
      remoteWrite: []
      replicaCount: 1
      resources: {}
      retention: 2d
      routePrefix: /
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: false
        runAsUser: 0
      service:
        annotations: {}
        clusterIP: ""
        enabled: true
        externalIPs: []
        gRPC:
          enabled: false
          servicePort: 10901
        labels: {}
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 80
        sessionAffinity: None
        statefulsetReplica:
          enabled: false
          replica: 0
        type: ClusterIP
      statefulSet:
        enabled: true
      tolerations:
      - effect: NoSchedule
        key: kb-controller
        operator: Equal
        value: "true"
    serverFiles:
      prometheus.yml:
        rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
        - /etc/config/kubelet_alert_rules.yml
        - /etc/config/mysql_alert_rules.yml
        - /etc/config/postgresql_alert_rules.yml
        - /etc/config/redis_alert_rules.yml
        - /etc/config/kafka_alert_rules.yml
        - /etc/config/mongodb_alert_rules.yml
        scrape_configs:
        - job_name: prometheus
          static_configs:
          - targets:
            - localhost:9090
        - honor_labels: true
          job_name: kubeblocks-service
          kubernetes_sd_configs:
          - role: endpoints
          relabel_configs:
          - action: keep
            regex: kubeblocks
            source_labels:
            - __meta_kubernetes_service_label_app_kubernetes_io_managed_by
          - action: drop
            regex: agamotto
            source_labels:
            - __meta_kubernetes_service_label_monitor_kubeblocks_io_managed_by
          - action: keep
            regex: true
            source_labels:
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_scrape
          - action: replace
            regex: (https?)
            source_labels:
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_scheme
            target_label: __scheme__
          - action: replace
            regex: (.+)
            source_labels:
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_path
            target_label: __metrics_path__
          - action: replace
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels:
            - __address__
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_port
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_service_annotation_monitor_kubeblocks_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - action: replace
            source_labels:
            - __meta_kubernetes_namespace
            target_label: namespace
          - action: replace
            source_labels:
            - __meta_kubernetes_service_name
            target_label: service
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_node_name
            target_label: node
          - action: replace
            source_labels:
            - __meta_kubernetes_pod_name
            target_label: pod
          - action: drop
            regex: Pending|Succeeded|Failed|Completed
            source_labels:
            - __meta_kubernetes_pod_phase
        - honor_labels: true
          job_name: kubeblocks-agamotto
          kubernetes_sd_configs:
          - role: endpoints
          relabel_configs:
          - action: keep
            regex: agamotto
            source_labels:
            - __meta_kubernetes_service_label_monitor_kubeblocks_io_managed_by
          - action: keep
            regex: true
            source_labels:
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_scrape
          - action: replace
            regex: (https?)
            source_labels:
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_scheme
            target_label: __scheme__
          - action: replace
            regex: (.+)
            source_labels:
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_path
            target_label: __metrics_path__
          - action: replace
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels:
            - __address__
            - __meta_kubernetes_service_annotation_monitor_kubeblocks_io_port
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_service_annotation_monitor_kubeblocks_io_param_(.+)
            replacement: __param_$1
          - action: drop
            regex: Pending|Succeeded|Failed|Completed
            source_labels:
            - __meta_kubernetes_pod_phase
